{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "import matplotlib.animation as animation\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "from scipy.stats import kruskal\n",
    "import graphviz as gr\n",
    "import math\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type1_learners = [7.0,8.0,9.0,10.0,12.0,25.0,30.0,31.0,32.0,38.0,40.0,42.0,44.0,47.0]  #Expressive Explorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'path/to/dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading data\n",
    "behavioral_data = pd.read_csv('PE-HRI_behavioral_timeseries.csv')\n",
    "behavioral_data=behavioral_data.drop('Unnamed: 0',axis=1)\n",
    "behavioral_data=behavioral_data.dropna()\n",
    "behavioral_data=behavioral_data[behavioral_data['team'].isin(type1_learners)]\n",
    "behavioral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating the metadata\n",
    "meta_data=behavioral_data.loc[:,'team':'window']\n",
    "team_id = behavioral_data.loc[:,'team']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating features and their columns' names\n",
    "features = behavioral_data.loc[:,'T_add':'normalized_time']\n",
    "columns= features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizing behavioral features wit a MinMax Scaler\n",
    "def standarize(df):\n",
    "    standardiser = preprocessing.MinMaxScaler()\n",
    "    data = standardiser.fit_transform(df)\n",
    "    df =pd.DataFrame(data,columns= df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standarized data\n",
    "std_features=standarize(features)\n",
    "std_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(df,n_components=columns.shape[0]):\n",
    "    pca = decomposition.PCA(n_components=n_components)\n",
    "    data=pca.fit_transform(df)\n",
    "    df=pd.DataFrame(data)\n",
    "    return df,pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCAfeatures,varRatio= PCA(std_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "x = np.arange(1,varRatio.shape[0]+1,step=1)\n",
    "ax.bar(x,varRatio)\n",
    "plt.xlabel('Principle Components')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the elbow method on the proportion of explained variance, we consider the first 4 PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take the first 4 components\n",
    "features_afterPCA,var = PCA(std_features, n_components=4)\n",
    "features_afterPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pair Plot of the PCs\n",
    "sns.pairplot(features_afterPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering with k from [1,10]\n",
    "# compute inertia and silhouette_scores to choose optimal k\n",
    "sse = []\n",
    "s_score=[]\n",
    "list_k = np.arange(1,10)\n",
    "for k in list_k:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(features_afterPCA)\n",
    "    labels = kmeans.labels_\n",
    "    sse.append(kmeans.inertia_)\n",
    "    if(k!=1):\n",
    "        s_score.append(silhouette_score(features_afterPCA,labels))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance')\n",
    "plt.title('intertia to number of clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(2,10), s_score, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('silhouette score')\n",
    "plt.title('silhouette to number of clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that there are 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(features_afterPCA)\n",
    "labels = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## features and their labels\n",
    "clustered_features=pd.DataFrame(std_features)\n",
    "clustered_features = clustered_features.assign(label=pd.Series(labels).values)\n",
    "clustered_features = clustered_features.assign(team=pd.Series(team_id).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## size per cluster\n",
    "clusters= clustered_features.groupby('label')\n",
    "size_per_cluster =clusters.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## conditional means\n",
    "mk = clusters.mean()\n",
    "mk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster 0\n",
    "clusterArray=[]\n",
    "cluster0 = clusters.get_group(0).drop('label',axis='columns')\n",
    "clusterArray.append(cluster0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster 1\n",
    "cluster1 = clusters.get_group(1).drop('label',axis='columns')\n",
    "clusterArray.append(cluster1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kruskal Wallis test on the clusters\n",
    "clus01=[]\n",
    "\n",
    "for feature in columns:\n",
    "    clus01.append((feature,kruskal(clusterArray[0][feature],clusterArray[1][feature]).pvalue))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus01.sort(key= lambda elem : elem[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorted by ascendant p-value/// features with pvalue > threshold are marked with xx\n",
    "threshold=0.01\n",
    "print (' ---- for cluster 0 and cluster 1 :')\n",
    "for pair in clus01:\n",
    "    ind=''\n",
    "    if(pair[1]>threshold):\n",
    "        ind=' xx '\n",
    "    print(' ------'+ind+' feature:'+str(pair[0])+'  || pvalue : '+str(pair[1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_feature01 =list(map(lambda c: c[0],list(filter(lambda pt: pt[1]<threshold,clus01))))\n",
    "mean0=list()\n",
    "mean1=list()\n",
    "for ft in significant_feature01:\n",
    "    mean0.append(mk[ft][0])\n",
    "    mean1.append(mk[ft][1])\n",
    "x = np.arange(len(significant_feature01))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "rects1 = ax.bar(x - width/2, mean0, width, label='Cluster0')\n",
    "rects2 = ax.bar(x + width/2, mean1, width, label='Cluster1')\n",
    "\n",
    "ax.set_ylabel('Means')\n",
    "ax.set_title('Means for significant features per cluster')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(significant_feature01)\n",
    "plt.xticks(rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each team the set of observations is a sequence\n",
    "## So we have #teams sequences with #dp_per_team emissions\n",
    "## label and team_n should be dropped and obseravtions should be sorted by time\n",
    "by_team = clustered_features.drop('label',axis=1).groupby('team')\n",
    "lengths= by_team.count()['normalized_time'].to_numpy()\n",
    "observations = by_team.get_group(7).drop('team',axis=1).to_numpy()\n",
    "for name,group in by_team:\n",
    "    if(name!=7):\n",
    "        observations = np.concatenate([observations,group.drop('team',axis=1).to_numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build HMM model with 3 components and fit it to the observations sequences\n",
    "model = hmm.GMMHMM(n_components=3,covariance_type='spherical',random_state= random.randint(0,100))\n",
    "model = model.fit(observations,lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Indicates whether the model has converged\n",
    "print('The model has converged: {}'.format(model.monitor_.converged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial state occupation distribution.\n",
    "model.startprob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrix of transition probabilities between states.\n",
    "model.transmat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict each dp state\n",
    "states=[]\n",
    "for name,group in by_team :\n",
    "    obs= group.drop('team',axis=1).to_numpy()\n",
    "    states =  np.concatenate([states,model.predict(obs,lengths=[obs.shape[0]])])\n",
    "states_d= pd.DataFrame(std_features).assign(state=pd.Series(states).values)\n",
    "states_dis = states_d.groupby('state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state0 = states_dis.get_group(0)\n",
    "state1 = states_dis.get_group(1)\n",
    "state2 = states_dis.get_group(2)\n",
    "states_dis.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Behaviors analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states= list(range(3))\n",
    "## Mean parameters for each mixture component in each state.\n",
    "fig, axs = plt.subplots(math.ceil(columns.shape[0]/4.0),4,figsize=(15,20))\n",
    "for i in range(len(model.means_[0][0])):\n",
    "    axs[i//4,i%4].bar(states,model.means_[:,0,i])\n",
    "    axs[i//4,i%4].set_ylabel('Means')\n",
    "    axs[i//4,i%4].set_title('Means for feature : '+columns[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues01 = []\n",
    "pvalues02= []\n",
    "pvalues12 = []\n",
    "pvalues012=[]\n",
    "for feature in columns:\n",
    "    try:\n",
    "        pvalues01.append(kruskal(state0[feature],state1[feature]).pvalue)\n",
    "    except:\n",
    "        pvalues01.append(None)\n",
    "    try :\n",
    "        pvalues02.append(kruskal(state0[feature],state2[feature]).pvalue)\n",
    "    except :\n",
    "        pvalues02.append(None)\n",
    "    try:\n",
    "        pvalues12.append(kruskal(state1[feature],state2[feature]).pvalue)\n",
    "    except:\n",
    "        pvalues12.append(None)\n",
    "    try:\n",
    "        pvalues012.append(kruskal(state0[feature],state1[feature],state2[feature]).pvalue)\n",
    "    except:\n",
    "        pvalues012.append(None)\n",
    "tests_df = pd.DataFrame(index=columns).assign(pvalue_01=pd.Series(pvalues01).values).assign(pvalue_02=pd.Series(pvalues02).values)\n",
    "tests_df = tests_df.assign(pvalue_12=pd.Series(pvalues12).values).assign(pvalue_012=pd.Series(pvalues012).values)\n",
    "tests_df = tests_df.assign(mean_0=pd.Series(model.means_[0][0]).values).assign(mean_1=pd.Series(model.means_[1][0]).values)\n",
    "tests_df = tests_df.assign(mean_2=pd.Series(model.means_[2][0]).values)\n",
    "tests_df=tests_df.sort_values('pvalue_012')\n",
    "tests_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered= tests_df[tests_df['pvalue_01']<0.01]\n",
    "filtered=filtered.sort_values('pvalue_01')\n",
    "significant = filtered.index.to_list()\n",
    "mean0=filtered['mean_0'].to_list()\n",
    "mean1=filtered['mean_1'].to_list()\n",
    "x = np.arange(len(significant))\n",
    "width = 0.35  # the width of the bars\n",
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "rects1 = ax.bar(x - width/2, mean0, width, label='State0')\n",
    "rects1 = ax.bar(x + width/2, mean1, width, label='State1')\n",
    "ax.set_ylabel('Means')\n",
    "ax.set_title('Means for significant features per state')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(significant)\n",
    "plt.xticks(rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered= tests_df[tests_df['pvalue_02']<0.01]\n",
    "filtered=filtered.sort_values('pvalue_02')\n",
    "significant = filtered.index.to_list()\n",
    "mean0=filtered['mean_0'].to_list()\n",
    "mean2=filtered['mean_2'].to_list()\n",
    "x = np.arange(len(significant))\n",
    "width = 0.35  # the width of the bars\n",
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "rects0 = ax.bar(x - width/2, mean0, width, label='State0')\n",
    "rects2 = ax.bar(x + width/2, mean2, width, label='State2')\n",
    "ax.set_ylabel('Means')\n",
    "ax.set_title('Means for significant features per state')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(significant)\n",
    "plt.xticks(rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered= tests_df[tests_df['pvalue_12']<0.01]\n",
    "filtered=filtered.sort_values('pvalue_12')\n",
    "significant = filtered.index.to_list()\n",
    "mean1=filtered['mean_1'].to_list()\n",
    "mean2=filtered['mean_2'].to_list()\n",
    "x = np.arange(len(significant))\n",
    "width = 0.35  # the width of the bars\n",
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "rects1 = ax.bar(x - width/2, mean1, width, label='State1')\n",
    "rects2 = ax.bar(x + width/2, mean2, width, label='State2')\n",
    "ax.set_ylabel('Means')\n",
    "ax.set_title('Means for significant features per state')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(significant)\n",
    "plt.xticks(rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered= tests_df[tests_df['pvalue_012']<0.01]\n",
    "significant = filtered.index.to_list()\n",
    "mean0=filtered['mean_0'].to_list()\n",
    "mean1=filtered['mean_1'].to_list()\n",
    "mean2=filtered['mean_2'].to_list()\n",
    "\n",
    "x = np.arange(len(significant))\n",
    "x=x+0.5# the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "rects1 = ax.bar(x - width, mean0, width, label='State0')\n",
    "rects1 = ax.bar(x , mean1, width, label='State1')\n",
    "rects2 = ax.bar(x + width, mean2, width, label='State2')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Means')\n",
    "ax.set_title('Means for significant features per state')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(significant)\n",
    "plt.xticks(rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate state description and transitions graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=['T_add', 'T_remove', 'T_ratio_add_del', 'T_action', 'T_hist', 'T_help',\n",
    "       'T1_T1_rem', 'T1_T1_add', 'T1_T2_rem', 'T1_T2_add', 'redundant_exist']\n",
    "video=['positive', 'negative', 'arousal', 'positive_minus_negative', 'smile',\n",
    "       'screen_left', 'screen_right', 'at_partner', 'at_robot', 'other']\n",
    "audio=['T_speech', 'T_silence', 'T_overlap', 'T_short_pauses', 'T_long_pauses',\n",
    "       'T_overlap_over_speech']\n",
    "sort_by=filtered.index.map(lambda x: 0 if(x in log) else (1 if(x in video) else (2 if(x in audio) else 3)))\n",
    "graph = filtered.assign(sort_by=sort_by)\n",
    "graph=graph.sort_values('sort_by',axis=0)\n",
    "graph=graph.drop('smile',axis=0)\n",
    "description = ['''Highest''','''High''','''Medium''','''Low''','''Lowest''']\n",
    "mean0=graph['mean_0'].to_list()\n",
    "mean1=graph['mean_1'].to_list()\n",
    "mean2=graph['mean_2'].to_list()\n",
    "means=[mean0,mean1,mean2]\n",
    "significant=graph.index.to_list()\n",
    "colors= ['''''','''''','''''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate state description\n",
    "# Highest Value for each feature is marked with highest, same for lowest\n",
    "# the remaining value is High, Medium, Low based on its distance from the min and the max\n",
    "states_des=[[\"\",\"\",\"\"],[\"\",\"\",\"\"],[\"\",\"\",\"\"],[\"\",\"\",\"\"]]\n",
    "new_des=[\"\",\"\",\"\"]\n",
    "i=0\n",
    "for ft in significant:\n",
    "    max_value= max(means[0][i],means[1][i],means[2][i])\n",
    "    min_value=min(means[0][i],means[1][i],means[2][i])\n",
    "    for j in range(3):\n",
    "        if(means[j][i]==max_value):\n",
    "            new_des[j]= \"{:<30}\".format(ft) +'\\t'+ description[0] +\"<br/>\"\n",
    "        elif(((means[j][i]-min_value)/(max_value-min_value))>2/3):\n",
    "            new_des[j]= \"{:<30}\".format(ft) +'\\t'+ description[1] +\"<br/>\"\n",
    "        elif (((means[j][i]-min_value)/(max_value-min_value))>1/3):\n",
    "            new_des[j]= \"{:<30}\".format(ft) +'\\t'+ description[2] +\"<br/>\"\n",
    "        elif(means[j][i]==min_value):\n",
    "            new_des[j]= \"{:<30}\".format(ft) +'\\t'+ description[4] +\"<br/>\"\n",
    "        else:\n",
    "            new_des[j]= \"{:<30}\".format(ft)+'\\t'+ description[3] +\"<br/>\"\n",
    "    if not(new_des[0]==new_des[1] and new_des[0]==new_des[2] and new_des[2]==new_des[1]):\n",
    "        for j in range(3):\n",
    "            states_des[graph.loc[ft,'sort_by']][j]+=new_des[j]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G= gr.Digraph('Type1HMMStateDiagram',format='jpeg')\n",
    "G.attr('graph',pad='1',ranksep='1',nodesep='1')\n",
    "prob=\"{proba:.2e}\"\n",
    "\n",
    "widths= [4,4,4]\n",
    "widths[np.argmin(model.startprob_)] = 0.5\n",
    "widths[np.argmax(model.startprob_)]=6\n",
    "\n",
    "G.attr('node',color='red')\n",
    "G.node('0.0','''<<font color=\"blue\">''' + states_des[0][0]+'''</font>'''\\\n",
    "       +'''<font color=\"#1d8348\">'''+ states_des[1][0]+'''</font>'''\\\n",
    "       +'''<font color=\"orange\">'''+ states_des[2][0]+'''</font>'''\\\n",
    "       '''<font color=\"black\">'''+ states_des[3][0]+'''</font>>''',shape='box')\n",
    "\n",
    "G.node('1.0','''<<font color=\"blue\">''' + states_des[0][1]+'''</font>'''\\\n",
    "       +'''<font color=\"#1d8348\">'''+ states_des[1][1]+'''</font>'''\\\n",
    "       +'''<font color=\"orange\">'''+ states_des[2][1]+'''</font>'''\\\n",
    "       '''<font color=\"black\">'''+ states_des[3][1]+'''</font>>''',shape='box')\n",
    "\n",
    "G.node('2.0','''<<font color=\"blue\">''' + states_des[0][2]+'''</font>'''\\\n",
    "       +'''<font color=\"#1d8348\">'''+ states_des[1][2]+'''</font>'''\\\n",
    "       +'''<font color=\"orange\">'''+ states_des[2][2]+'''</font>'''\\\n",
    "       '''<font color=\"black\">'''+ states_des[3][2]+'''</font>>''',shape='box')\n",
    "\n",
    "G.attr('node',penwidth=str(widths[0]))\n",
    "G.node('0','State 0\\n\\nInitial state probability: '+prob.format(proba=model.startprob_[0]))\n",
    "G.attr('node',penwidth=str(widths[1]))\n",
    "G.node('1','State 1\\n\\nInitial state probability: '+prob.format(proba=model.startprob_[1]))\n",
    "G.attr('node',penwidth=str(widths[2]))\n",
    "G.node('2','State 2\\n\\nInitial state probability: '+prob.format(proba=model.startprob_[2]))\n",
    "\n",
    "\n",
    "prob=\"{proba:.3f}\"\n",
    "G.attr('edge',penwidth=str(10*model.transmat_[0][0]))\n",
    "G.edge('0','0',prob.format(proba=model.transmat_[0][0]))\n",
    "G.attr('edge',penwidth=str(10*model.transmat_[0][1]))\n",
    "G.edge('0','1',prob.format(proba=model.transmat_[0][1]))\n",
    "G.attr('edge',penwidth=str(10*model.transmat_[0][2]))\n",
    "G.edge('0','2',prob.format(proba=model.transmat_[0][2]))\n",
    "\n",
    "G.attr('edge',penwidth=str(10*model.transmat_[1][0]))\n",
    "G.edge('1','0',prob.format(proba=model.transmat_[1][0]))\n",
    "G.attr('edge',penwidth=str(10*model.transmat_[1][1]))\n",
    "G.edge('1','1',prob.format(proba=model.transmat_[1][1]))\n",
    "G.attr('edge',penwidth=str(10*model.transmat_[1][2]))\n",
    "G.edge('1','2',prob.format(proba=model.transmat_[1][2]))\n",
    "\n",
    "G.attr('edge',penwidth=str(10*model.transmat_[2][0]))\n",
    "G.edge('2','0',prob.format(proba=model.transmat_[2][0]))\n",
    "G.attr('edge',penwidth=str(10*model.transmat_[2][1]))\n",
    "G.edge('2','1',prob.format(proba=model.transmat_[2][1]))\n",
    "G.attr('edge',penwidth=str(10*model.transmat_[2][2]))\n",
    "G.edge('2','2',prob.format(proba=model.transmat_[2][2]))\n",
    "\n",
    "G.attr('edge',penwidth=str(1))\n",
    "G.attr('edge',fontcolor='red')\n",
    "G.attr('edge',dir='none')\n",
    "G.attr('edge',color='red')\n",
    "G.edge('0','0.0','Key Features')\n",
    "G.edge('1','1.0','Key Features')\n",
    "G.edge('2','2.0','Key Features')\n",
    "G.graph_attr.update(size=\"15,12\")\n",
    "\n",
    "G.view()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
